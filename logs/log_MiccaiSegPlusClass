>>>>>>>>>>>>>>>>>>>>>>>Training<<<<<<<<<<<<<<<<<<<<<<<
[68/89][0/122] Total Loss: 0.9920, Segmentation Loss: 0.5531, Classification Loss: 0.4390
[68/89][1/122] Total Loss: 1.1736, Segmentation Loss: 0.5918, Classification Loss: 0.5818
[68/89][2/122] Total Loss: 1.0042, Segmentation Loss: 0.5652, Classification Loss: 0.4390
[68/89][3/122] Total Loss: 0.9628, Segmentation Loss: 0.5238, Classification Loss: 0.4390
[68/89][4/122] Total Loss: 1.0884, Segmentation Loss: 0.5066, Classification Loss: 0.5818
[68/89][5/122] Total Loss: 1.0071, Segmentation Loss: 0.4967, Classification Loss: 0.5104
[68/89][6/122] Total Loss: 1.5041, Segmentation Loss: 0.6365, Classification Loss: 0.8675
[68/89][7/122] Total Loss: 1.4546, Segmentation Loss: 0.6585, Classification Loss: 0.7961
[68/89][8/122] Total Loss: 1.0550, Segmentation Loss: 0.4732, Classification Loss: 0.5818
[68/89][9/122] Total Loss: 0.8238, Segmentation Loss: 0.4562, Classification Loss: 0.3675
[68/89][10/122] Total Loss: 1.1896, Segmentation Loss: 0.6078, Classification Loss: 0.5818
[68/89][11/122] Total Loss: 1.2408, Segmentation Loss: 0.5876, Classification Loss: 0.6532
[68/89][12/122] Total Loss: 1.3865, Segmentation Loss: 0.5904, Classification Loss: 0.7961
[68/89][13/122] Total Loss: 0.8821, Segmentation Loss: 0.5146, Classification Loss: 0.3675
[68/89][14/122] Total Loss: 1.1704, Segmentation Loss: 0.5886, Classification Loss: 0.5818
[68/89][15/122] Total Loss: 0.8574, Segmentation Loss: 0.4899, Classification Loss: 0.3675
[68/89][16/122] Total Loss: 1.3847, Segmentation Loss: 0.7288, Classification Loss: 0.6560
[68/89][17/122] Total Loss: 1.2547, Segmentation Loss: 0.6014, Classification Loss: 0.6532
[68/89][18/122] Total Loss: 0.9909, Segmentation Loss: 0.5519, Classification Loss: 0.4390
[68/89][19/122] Total Loss: 1.0571, Segmentation Loss: 0.4753, Classification Loss: 0.5818
[68/89][20/122] Total Loss: 0.9464, Segmentation Loss: 0.5074, Classification Loss: 0.4390
[68/89][21/122] Total Loss: 1.0750, Segmentation Loss: 0.5646, Classification Loss: 0.5104
[68/89][22/122] Total Loss: 0.8226, Segmentation Loss: 0.4551, Classification Loss: 0.3675
[68/89][23/122] Total Loss: 1.1106, Segmentation Loss: 0.6716, Classification Loss: 0.4390
[68/89][24/122] Total Loss: 1.3548, Segmentation Loss: 0.6301, Classification Loss: 0.7247
[68/89][25/122] Total Loss: 1.7893, Segmentation Loss: 0.7789, Classification Loss: 1.0104
[68/89][26/122] Total Loss: 1.2219, Segmentation Loss: 0.4972, Classification Loss: 0.7247
[68/89][27/122] Total Loss: 1.0781, Segmentation Loss: 0.5677, Classification Loss: 0.5104
[68/89][28/122] Total Loss: 0.8449, Segmentation Loss: 0.4774, Classification Loss: 0.3675
[68/89][29/122] Total Loss: 1.2000, Segmentation Loss: 0.5467, Classification Loss: 0.6532
[68/89][30/122] Total Loss: 0.9980, Segmentation Loss: 0.5591, Classification Loss: 0.4390
[68/89][31/122] Total Loss: 0.9368, Segmentation Loss: 0.4978, Classification Loss: 0.4390
[68/89][32/122] Total Loss: 1.2727, Segmentation Loss: 0.6195, Classification Loss: 0.6532
[68/89][33/122] Total Loss: 1.5091, Segmentation Loss: 0.6416, Classification Loss: 0.8675
[68/89][34/122] Total Loss: 1.2684, Segmentation Loss: 0.6152, Classification Loss: 0.6532
[68/89][35/122] Total Loss: 1.3805, Segmentation Loss: 0.6558, Classification Loss: 0.7247
[68/89][36/122] Total Loss: 1.4196, Segmentation Loss: 0.6235, Classification Loss: 0.7961
[68/89][37/122] Total Loss: 1.2301, Segmentation Loss: 0.5768, Classification Loss: 0.6532
[68/89][38/122] Total Loss: 1.0707, Segmentation Loss: 0.5604, Classification Loss: 0.5104
[68/89][39/122] Total Loss: 1.1660, Segmentation Loss: 0.5842, Classification Loss: 0.5818
[68/89][40/122] Total Loss: 1.1338, Segmentation Loss: 0.5520, Classification Loss: 0.5818
[68/89][41/122] Total Loss: 0.8229, Segmentation Loss: 0.4554, Classification Loss: 0.3675
[68/89][42/122] Total Loss: 1.0901, Segmentation Loss: 0.5797, Classification Loss: 0.5104
[68/89][43/122] Total Loss: 1.0985, Segmentation Loss: 0.5167, Classification Loss: 0.5818
[68/89][44/122] Total Loss: 1.2630, Segmentation Loss: 0.6098, Classification Loss: 0.6532
[68/89][45/122] Total Loss: 1.0721, Segmentation Loss: 0.5617, Classification Loss: 0.5104
[68/89][46/122] Total Loss: 0.9917, Segmentation Loss: 0.5528, Classification Loss: 0.4390
[68/89][47/122] Total Loss: 1.2679, Segmentation Loss: 0.6147, Classification Loss: 0.6532
[68/89][48/122] Total Loss: 1.0485, Segmentation Loss: 0.6095, Classification Loss: 0.4390
[68/89][49/122] Total Loss: 1.1679, Segmentation Loss: 0.5146, Classification Loss: 0.6532
[68/89][50/122] Total Loss: 0.9383, Segmentation Loss: 0.4993, Classification Loss: 0.4390
[68/89][51/122] Total Loss: 1.4230, Segmentation Loss: 0.6269, Classification Loss: 0.7961
[68/89][52/122] Total Loss: 1.1760, Segmentation Loss: 0.5227, Classification Loss: 0.6532
[68/89][53/122] Total Loss: 1.4496, Segmentation Loss: 0.6535, Classification Loss: 0.7961
[68/89][54/122] Total Loss: 1.2657, Segmentation Loss: 0.6125, Classification Loss: 0.6532
[68/89][55/122] Total Loss: 1.2941, Segmentation Loss: 0.5694, Classification Loss: 0.7247
[68/89][56/122] Total Loss: 0.8904, Segmentation Loss: 0.5229, Classification Loss: 0.3675
[68/89][57/122] Total Loss: 1.1180, Segmentation Loss: 0.5361, Classification Loss: 0.5818
[68/89][58/122] Total Loss: 0.9793, Segmentation Loss: 0.5403, Classification Loss: 0.4390
[68/89][59/122] Total Loss: 1.0501, Segmentation Loss: 0.5397, Classification Loss: 0.5104
[68/89][60/122] Total Loss: 1.2000, Segmentation Loss: 0.6182, Classification Loss: 0.5818
[68/89][61/122] Total Loss: 1.2644, Segmentation Loss: 0.5398, Classification Loss: 0.7247
[68/89][62/122] Total Loss: 1.3096, Segmentation Loss: 0.6563, Classification Loss: 0.6532
[68/89][63/122] Total Loss: 1.1916, Segmentation Loss: 0.6098, Classification Loss: 0.5818
[68/89][64/122] Total Loss: 1.1645, Segmentation Loss: 0.5113, Classification Loss: 0.6532
[68/89][65/122] Total Loss: 0.8798, Segmentation Loss: 0.5122, Classification Loss: 0.3675
[68/89][66/122] Total Loss: 1.2999, Segmentation Loss: 0.5753, Classification Loss: 0.7247
[68/89][67/122] Total Loss: 1.0625, Segmentation Loss: 0.5521, Classification Loss: 0.5104
[68/89][68/122] Total Loss: 1.1061, Segmentation Loss: 0.5243, Classification Loss: 0.5818
[68/89][69/122] Total Loss: 1.4663, Segmentation Loss: 0.5988, Classification Loss: 0.8675
[68/89][70/122] Total Loss: 1.0437, Segmentation Loss: 0.5333, Classification Loss: 0.5104
[68/89][71/122] Total Loss: 0.9632, Segmentation Loss: 0.5242, Classification Loss: 0.4390
[68/89][72/122] Total Loss: 1.0697, Segmentation Loss: 0.5593, Classification Loss: 0.5104
[68/89][73/122] Total Loss: 1.0829, Segmentation Loss: 0.5725, Classification Loss: 0.5104
[68/89][74/122] Total Loss: 1.1908, Segmentation Loss: 0.6090, Classification Loss: 0.5818
[68/89][75/122] Total Loss: 1.0636, Segmentation Loss: 0.4818, Classification Loss: 0.5818
[68/89][76/122] Total Loss: 0.9226, Segmentation Loss: 0.5551, Classification Loss: 0.3675
[68/89][77/122] Total Loss: 1.4255, Segmentation Loss: 0.6294, Classification Loss: 0.7961
[68/89][78/122] Total Loss: 1.0131, Segmentation Loss: 0.5741, Classification Loss: 0.4390
[68/89][79/122] Total Loss: 1.0155, Segmentation Loss: 0.5051, Classification Loss: 0.5104
[68/89][80/122] Total Loss: 1.3562, Segmentation Loss: 0.5601, Classification Loss: 0.7961
[68/89][81/122] Total Loss: 0.9642, Segmentation Loss: 0.5253, Classification Loss: 0.4390
[68/89][82/122] Total Loss: 1.1435, Segmentation Loss: 0.5617, Classification Loss: 0.5818
[68/89][83/122] Total Loss: 1.1661, Segmentation Loss: 0.6558, Classification Loss: 0.5104
[68/89][84/122] Total Loss: 0.8777, Segmentation Loss: 0.5102, Classification Loss: 0.3675
[68/89][85/122] Total Loss: 0.9349, Segmentation Loss: 0.4959, Classification Loss: 0.4390
[68/89][86/122] Total Loss: 1.0643, Segmentation Loss: 0.5539, Classification Loss: 0.5104
[68/89][87/122] Total Loss: 1.3621, Segmentation Loss: 0.5660, Classification Loss: 0.7961
[68/89][88/122] Total Loss: 1.1963, Segmentation Loss: 0.5431, Classification Loss: 0.6532
[68/89][89/122] Total Loss: 1.1396, Segmentation Loss: 0.6292, Classification Loss: 0.5104
[68/89][90/122] Total Loss: 1.2514, Segmentation Loss: 0.5982, Classification Loss: 0.6532
[68/89][91/122] Total Loss: 0.9785, Segmentation Loss: 0.5395, Classification Loss: 0.4390
[68/89][92/122] Total Loss: 1.2814, Segmentation Loss: 0.5567, Classification Loss: 0.7247
[68/89][93/122] Total Loss: 1.1670, Segmentation Loss: 0.5138, Classification Loss: 0.6532
[68/89][94/122] Total Loss: 1.4854, Segmentation Loss: 0.6179, Classification Loss: 0.8675
[68/89][95/122] Total Loss: 1.2276, Segmentation Loss: 0.6457, Classification Loss: 0.5818
[68/89][96/122] Total Loss: 1.1601, Segmentation Loss: 0.5783, Classification Loss: 0.5818
[68/89][97/122] Total Loss: 1.1227, Segmentation Loss: 0.5408, Classification Loss: 0.5818
[68/89][98/122] Total Loss: 1.2788, Segmentation Loss: 0.5541, Classification Loss: 0.7247
[68/89][99/122] Total Loss: 1.7134, Segmentation Loss: 0.7744, Classification Loss: 0.9390
[68/89][100/122] Total Loss: 1.2321, Segmentation Loss: 0.5789, Classification Loss: 0.6532
[68/89][101/122] Total Loss: 1.1854, Segmentation Loss: 0.6036, Classification Loss: 0.5818
[68/89][102/122] Total Loss: 0.8885, Segmentation Loss: 0.4496, Classification Loss: 0.4390
[68/89][103/122] Total Loss: 1.3967, Segmentation Loss: 0.6720, Classification Loss: 0.7247
[68/89][104/122] Total Loss: 0.9195, Segmentation Loss: 0.5520, Classification Loss: 0.3675
[68/89][105/122] Total Loss: 1.0349, Segmentation Loss: 0.5245, Classification Loss: 0.5104
[68/89][106/122] Total Loss: 1.4897, Segmentation Loss: 0.6222, Classification Loss: 0.8675
[68/89][107/122] Total Loss: 1.6472, Segmentation Loss: 0.7796, Classification Loss: 0.8675
[68/89][108/122] Total Loss: 1.0051, Segmentation Loss: 0.4947, Classification Loss: 0.5104
[68/89][109/122] Total Loss: 1.3410, Segmentation Loss: 0.4735, Classification Loss: 0.8675
[68/89][110/122] Total Loss: 1.1141, Segmentation Loss: 0.5323, Classification Loss: 0.5818
[68/89][111/122] Total Loss: 1.0006, Segmentation Loss: 0.5616, Classification Loss: 0.4390
[68/89][112/122] Total Loss: 0.9902, Segmentation Loss: 0.5512, Classification Loss: 0.4390
[68/89][113/122] Total Loss: 1.1080, Segmentation Loss: 0.5976, Classification Loss: 0.5104
[68/89][114/122] Total Loss: 1.3232, Segmentation Loss: 0.5985, Classification Loss: 0.7247
[68/89][115/122] Total Loss: 1.3079, Segmentation Loss: 0.5832, Classification Loss: 0.7247
[68/89][116/122] Total Loss: 1.0259, Segmentation Loss: 0.5869, Classification Loss: 0.4390
[68/89][117/122] Total Loss: 1.2315, Segmentation Loss: 0.5783, Classification Loss: 0.6532
[68/89][118/122] Total Loss: 1.0504, Segmentation Loss: 0.4686, Classification Loss: 0.5818
[68/89][119/122] Total Loss: 1.3281, Segmentation Loss: 0.6034, Classification Loss: 0.7247
[68/89][120/122] Total Loss: 0.8992, Segmentation Loss: 0.4603, Classification Loss: 0.4390
[68/89][121/122] Total Loss: 1.0822, Segmentation Loss: 0.5004, Classification Loss: 0.5818
[68/89][122/122] Total Loss: 0.9224, Segmentation Loss: 0.5548, Classification Loss: 0.3675
>>>>>>>>>>>>>>>>>>>>>>>Testing<<<<<<<<<<<<<<<<<<<<<<<
[68/89][0/30] Total Loss: 1.7958, Segmentation Loss: 0.7854, Classification Loss: 1.0104
[68/89][1/30] Total Loss: 1.5154, Segmentation Loss: 0.7907, Classification Loss: 0.7247
[68/89][2/30] Total Loss: 1.5500, Segmentation Loss: 0.6824, Classification Loss: 0.8675
[68/89][3/30] Total Loss: 1.3377, Segmentation Loss: 0.7559, Classification Loss: 0.5818
[68/89][4/30] Total Loss: 1.4290, Segmentation Loss: 0.7757, Classification Loss: 0.6532
[68/89][5/30] Total Loss: 1.6458, Segmentation Loss: 0.8497, Classification Loss: 0.7961
[68/89][6/30] Total Loss: 1.4272, Segmentation Loss: 0.7739, Classification Loss: 0.6532
[68/89][7/30] Total Loss: 1.1951, Segmentation Loss: 0.6847, Classification Loss: 0.5104
[68/89][8/30] Total Loss: 1.3791, Segmentation Loss: 0.7259, Classification Loss: 0.6532
[68/89][9/30] Total Loss: 1.6178, Segmentation Loss: 0.8217, Classification Loss: 0.7961
[68/89][10/30] Total Loss: 1.7179, Segmentation Loss: 0.7790, Classification Loss: 0.9390
[68/89][11/30] Total Loss: 1.5453, Segmentation Loss: 0.6778, Classification Loss: 0.8675
[68/89][12/30] Total Loss: 1.6476, Segmentation Loss: 0.7801, Classification Loss: 0.8675
[68/89][13/30] Total Loss: 1.8546, Segmentation Loss: 0.8443, Classification Loss: 1.0104
[68/89][14/30] Total Loss: 1.6373, Segmentation Loss: 0.8412, Classification Loss: 0.7961
[68/89][15/30] Total Loss: 1.8082, Segmentation Loss: 0.7978, Classification Loss: 1.0104
[68/89][16/30] Total Loss: 1.3730, Segmentation Loss: 0.7197, Classification Loss: 0.6532
[68/89][17/30] Total Loss: 1.3457, Segmentation Loss: 0.6925, Classification Loss: 0.6532
[68/89][18/30] Total Loss: 1.2954, Segmentation Loss: 0.6422, Classification Loss: 0.6532
[68/89][19/30] Total Loss: 1.6632, Segmentation Loss: 0.7242, Classification Loss: 0.9390
[68/89][20/30] Total Loss: 1.6942, Segmentation Loss: 0.7552, Classification Loss: 0.9390
[68/89][21/30] Total Loss: 1.5139, Segmentation Loss: 0.7178, Classification Loss: 0.7961
[68/89][22/30] Total Loss: 1.6411, Segmentation Loss: 0.8450, Classification Loss: 0.7961
[68/89][23/30] Total Loss: 1.5003, Segmentation Loss: 0.7042, Classification Loss: 0.7961
[68/89][24/30] Total Loss: 1.2520, Segmentation Loss: 0.7416, Classification Loss: 0.5104
[68/89][25/30] Total Loss: 1.5941, Segmentation Loss: 0.7980, Classification Loss: 0.7961
[68/89][26/30] Total Loss: 1.4057, Segmentation Loss: 0.7525, Classification Loss: 0.6532
[68/89][27/30] Total Loss: 1.4671, Segmentation Loss: 0.7424, Classification Loss: 0.7247
[68/89][28/30] Total Loss: 1.7646, Segmentation Loss: 0.8257, Classification Loss: 0.9390
[68/89][29/30] Total Loss: 1.6440, Segmentation Loss: 0.8479, Classification Loss: 0.7961
[68/89][30/30] Total Loss: 1.2425, Segmentation Loss: 0.7321, Classification Loss: 0.5104
>>>>>>>>>>>>>>>>>> Evaluating the Metrics <<<<<<<<<<<<<<<<<
Mean IoU: 0.26102980050830676, Class-wise IoU: tensor([0.0000, 0.4032, 0.0000, 0.5511, 0.0118, 0.5311, 0.0010, 0.2156, 0.0000,
        0.0000, 0.7155, 0.5270, 0.5012, 0.3961, 0.0540, 0.1426, 0.0000, 0.0000,
        0.9093], dtype=torch.float64)
Mean Precision: 0.3741856962618957, Class-wise Precision: tensor([0.0000, 0.5323, 0.0000, 0.6578, 0.0260, 0.7584, 0.0602, 0.2556, 0.0000,
        0.0000, 0.7753, 0.6425, 0.7226, 0.6833, 0.3265, 0.7332, 0.0000, 0.0000,
        0.9358], dtype=torch.float64)
Mean Recall: 0.3459443871241222, Class-wise Recall: tensor([0.0000, 0.6244, 0.0000, 0.7726, 0.0212, 0.6393, 0.0010, 0.5792, 0.0000,
        0.0000, 0.9028, 0.7456, 0.6207, 0.4853, 0.0607, 0.1504, 0.0000, 0.0000,
        0.9697], dtype=torch.float64)
Mean F1: 0.3380635847839582, Class-wise F1: tensor([0.0000, 0.5747, 0.0000, 0.7106, 0.0233, 0.6938, 0.0019, 0.3547, 0.0000,
        0.0000, 0.8342, 0.6902, 0.6678, 0.5675, 0.1024, 0.2496, 0.0000, 0.0000,
        0.9525], dtype=torch.float64)